{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Dengu u\n"
     ]
    }
   ],
   "source": [
    "print (\"Hello Dengu u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Naa istam'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Naa istam\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.178:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"hour_noheader.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour_noheader.csv MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2011-01-01', '1', '0', '1', '0', '0', '6', '0', '1', '0.24', '0.2879', '0.81', '0', '3', '13', '16']\n"
     ]
    }
   ],
   "source": [
    "print (first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2011-01-01',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '6',\n",
       " '0',\n",
       " '1',\n",
       " '0.24',\n",
       " '0.2879',\n",
       " '0.81',\n",
       " '0',\n",
       " '3',\n",
       " '13',\n",
       " '16']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feasture column: {'1': 0, '4': 1, '2': 2, '3': 3}\n"
     ]
    }
   ],
   "source": [
    "print (\"Mapping of first categorical feasture column: %s\" % (get_mapping(records, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 57 \n",
      "Feature vector length for numerical features: 4 \n",
      "Total feature vector length: 61 \n"
     ]
    }
   ],
   "source": [
    "print (\"Feature vector length for categorical features: %d \" % cat_len)\n",
    "print (\"Feature vector length for numerical features: %d \" % num_len)\n",
    "print (\"Total feature vector length: %d \" % total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.    , 0.    , 0.    , 1.    , 0.    , 1.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 1.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 1.    , 0.    , 0.    , 0.    , 0.    , 1.    ,\n",
       "       0.    , 0.    , 0.    , 1.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.24  , 0.2879, 0.81  , 0.    ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "extract_features(records.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_point = data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data['1', '0', '1', '0', '0', '6', '0', '1', '0.24', '0.2879', '0.81', '0', '3', '13', '16']\n",
      "Label:16.0\n",
      "Linear Model feature vector:\n",
      "[1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0]\n",
      "Linear Model feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "print (\"Raw Data\" + str(first[2:]))\n",
    "\n",
    "print (\"Label:\" + str(first_point.label))\n",
    "\n",
    "print (\"Linear Model feature vector:\\n\" + str(first_point.features))\n",
    "\n",
    "print (\"Linear Model feature vector length: \" + str(len(first_point.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree feature vector: [1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]\n",
      "Decision Tree feature vector length:12\n"
     ]
    }
   ],
   "source": [
    "# Extract features of decision tree. \n",
    "\n",
    "def extract_features_dt(record):\n",
    "    return np.array(list(map(float, record[2:14])))\n",
    "\n",
    "# def extract_features_dt(record):\n",
    "#     cat_vec = np.zeros(cat_len)\n",
    "#     i = 0\n",
    "#     step = 0\n",
    "#     for field in record[2:9]:\n",
    "#         m = mappings[i]\n",
    "#         idx = m[field]\n",
    "#         cat_vec[idx + step] = 1\n",
    "#         i = i + 1\n",
    "#         step = step + len(m)\n",
    "#     num_vec = np.array([float(field) for field in record[10:14]])\n",
    "#     return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "\n",
    "\n",
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "\n",
    "# label = extract_label(records.first())\n",
    "# print (label)\n",
    "# features_array = extract_features_dt(records.first())\n",
    "# print (features_array)\n",
    "\n",
    "# LabeledPoint(label, features_array)\n",
    "\n",
    "data_dt.count()\n",
    "\n",
    "first_point_dt = data_dt.first()\n",
    "\n",
    "print (\"Decision Tree feature vector: \"  + str(first_point_dt.features))\n",
    "\n",
    "print (\"Decision Tree feature vector length:\" + str(len(first_point_dt.features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'func': <function pyspark.rdd.RDD.map.<locals>.func(_, iterator)>,\n",
       " 'preservesPartitioning': False,\n",
       " '_prev_jrdd': JavaObject id=o57,\n",
       " '_prev_jrdd_deserializer': AutoBatchedSerializer(PickleSerializer()),\n",
       " 'is_cached': False,\n",
       " 'is_checkpointed': False,\n",
       " 'ctx': <SparkContext master=local[2] appName=PySparkShell>,\n",
       " 'prev': PythonRDD[5] at RDD at PythonRDD.scala:52,\n",
       " '_jrdd_val': JavaObject id=o777,\n",
       " '_id': None,\n",
       " '_jrdd_deserializer': AutoBatchedSerializer(PickleSerializer()),\n",
       " '_bypass_serializer': False,\n",
       " 'partitioner': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(16.0, 117.89250386724845), (40.0, 116.22496123192113), (32.0, 116.02369145779235), (13.0, 115.67088016754433), (1.0, 115.56315650834316)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Linear Model predictions: \" + str(true_vs_predicted.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]\n",
      "Decision Tree depth: 5\n",
      "Decision Tree number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "dt_model = DecisionTree.trainRegressor(data_dt,{})\n",
    "\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "\n",
    "actual = data.map(lambda p: p.label)\n",
    "\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "\n",
    "print (\"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5)))\n",
    "\n",
    "print (\"Decision Tree depth: \" + str(dt_model.depth()))\n",
    "\n",
    "print (\"Decision Tree number of nodes: \" + str(dt_model.numNodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(actual, predicted):\n",
    "    return (predicted - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model – Mean Squared Error: 30679.4539\n",
      "Linear Model – Mean Absolute Error: 130.6429\n",
      "Linear Model – Root Mean Squared Log Error: 1.4653\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Linear Model\n",
    "\n",
    "mse = true_vs_predicted.map(lambda args: squared_error(args[0], args[1])).mean()\n",
    "\n",
    "mae = true_vs_predicted.map(lambda args: abs_error(args[0], args[1])).mean()\n",
    "\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda args: squared_log_error(args[0], args[1])).mean())\n",
    "\n",
    "print (\"Linear Model – Mean Squared Error: %2.4f\" % mse)\n",
    "\n",
    "print (\"Linear Model – Mean Absolute Error: %2.4f\" % mae)\n",
    "\n",
    "print (\"Linear Model – Root Mean Squared Log Error: %2.4f\" % rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.2 Decision Tree\n",
    "\n",
    "\n",
    "mse_dt = true_vs_predicted_dt.map(lambda args: squared_error(args[0], args[1])).\n",
    "\n",
    "mean()\n",
    "\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).\n",
    "\n",
    "mean()\n",
    "\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_\n",
    "\n",
    "log_error(t, p)).mean())\n",
    "\n",
    "print “Decision Tree – Mean Squared Error: %2.4f” % mse_dt\n",
    "\n",
    "print “Decision Tree – Mean Absolute Error: %2.4f” % mae_dt\n",
    "\n",
    "print “Decision Tree – Root Mean Squared Log Error: %2.4f” %\n",
    "\n",
    "rmsle_dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
