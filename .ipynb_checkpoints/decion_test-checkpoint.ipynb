{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Dengu u\n"
     ]
    }
   ],
   "source": [
    "print (\"Hello Dengu u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Naa istam'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Naa istam\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-50eb3b0ac7ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hour_noheader.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "raw_data = sc.textFile(\"hour_noheader.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mapping of first categorical feasture column: %s\" % (get_mapping(records, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Feature vector length for categorical features: %d \" % cat_len)\n",
    "print (\"Feature vector length for numerical features: %d \" % num_len)\n",
    "print (\"Total feature vector length: %d \" % total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "extract_features(records.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_point = data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Raw Data\" + str(first[2:]))\n",
    "\n",
    "print (\"Label:\" + str(first_point.label))\n",
    "\n",
    "print (\"Linear Model feature vector:\\n\" + str(first_point.features))\n",
    "\n",
    "print (\"Linear Model feature vector length: \" + str(len(first_point.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features of decision tree. \n",
    "\n",
    "def extract_features_dt(record):\n",
    "    return np.array(list(map(float, record[2:14])))\n",
    "\n",
    "# def extract_features_dt(record):\n",
    "#     cat_vec = np.zeros(cat_len)\n",
    "#     i = 0\n",
    "#     step = 0\n",
    "#     for field in record[2:9]:\n",
    "#         m = mappings[i]\n",
    "#         idx = m[field]\n",
    "#         cat_vec[idx + step] = 1\n",
    "#         i = i + 1\n",
    "#         step = step + len(m)\n",
    "#     num_vec = np.array([float(field) for field in record[10:14]])\n",
    "#     return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "\n",
    "\n",
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "\n",
    "# label = extract_label(records.first())\n",
    "# print (label)\n",
    "# features_array = extract_features_dt(records.first())\n",
    "# print (features_array)\n",
    "\n",
    "# LabeledPoint(label, features_array)\n",
    "\n",
    "data_dt.count()\n",
    "\n",
    "first_point_dt = data_dt.first()\n",
    "\n",
    "print (\"Decision Tree feature vector: \"  + str(first_point_dt.features))\n",
    "\n",
    "print (\"Decision Tree feature vector length:\" + str(len(first_point_dt.features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Linear Model predictions: \" + str(true_vs_predicted.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "dt_model = DecisionTree.trainRegressor(data_dt,{})\n",
    "\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "\n",
    "actual = data.map(lambda p: p.label)\n",
    "\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "\n",
    "print (\"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5)))\n",
    "\n",
    "print (\"Decision Tree depth: \" + str(dt_model.depth()))\n",
    "\n",
    "print (\"Decision Tree number of nodes: \" + str(dt_model.numNodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(actual, predicted):\n",
    "    return (predicted - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Linear Model\n",
    "\n",
    "mse = true_vs_predicted.map(lambda args: squared_error(args[0], args[1])).mean()\n",
    "\n",
    "mae = true_vs_predicted.map(lambda args: abs_error(args[0], args[1])).mean()\n",
    "\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda args: squared_log_error(args[0], args[1])).mean())\n",
    "\n",
    "print (\"Linear Model – Mean Squared Error: %2.4f\" % mse)\n",
    "\n",
    "print (\"Linear Model – Mean Absolute Error: %2.4f\" % mae)\n",
    "\n",
    "print (\"Linear Model – Root Mean Squared Log Error: %2.4f\" % rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.2 Decision Tree\n",
    "\n",
    "\n",
    "mse_dt = true_vs_predicted_dt.map(lambda args: squared_error(args[0], args[1])).\n",
    "\n",
    "mean()\n",
    "\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).\n",
    "\n",
    "mean()\n",
    "\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_\n",
    "\n",
    "log_error(t, p)).mean())\n",
    "\n",
    "print “Decision Tree – Mean Squared Error: %2.4f” % mse_dt\n",
    "\n",
    "print “Decision Tree – Mean Absolute Error: %2.4f” % mae_dt\n",
    "\n",
    "print “Decision Tree – Root Mean Squared Log Error: %2.4f” %\n",
    "\n",
    "rmsle_dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
